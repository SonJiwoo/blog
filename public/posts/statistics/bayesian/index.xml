<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bayesian on JW Blog</title>
    <link>https://jiwooblog.netlify.app/posts/statistics/bayesian/</link>
    <description>Recent content in Bayesian on JW Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ko</language>
    <copyright>&amp;copy;{year}, All Rights Reserved</copyright>
    <lastBuildDate>Fri, 08 Jan 2021 10:08:56 +0900</lastBuildDate>
    
        <atom:link href="https://jiwooblog.netlify.app/posts/statistics/bayesian/index.xml" rel="self" type="application/rss+xml" />
    
    
    
      
      <item>
        <title>Gibbs Sampler</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/bayesian/gibbs_sampler/</link>
        <pubDate>Tue, 23 Feb 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/bayesian/gibbs_sampler/</guid>
        <description>Gibbs Sampler 기본 원리 (추후 업데이트)
종류 1. Systematic Sweep Gibbs Sampler 모든 모수에 대해 샘플링을 반복하여 업데이트하는 방법
2. Random Sweep Gibbs Sampler 무작위로 모수를 뽑아서 업데이트하는 방법
3. Grouped Gibbs Sampler 여러 개의 모수를 한번에 뽑아서 업데이트하는 방법
이때 모수 간의 연관성이 생긴다.
4. Collapsed Gibbs Sampler 적분을 통해서 특정 모수와 독립적인 분포를 계산한 후, 샘플링하여 업데이트하는 방법
참고사이트 [1] https://niceguy1575.tistory.com/entry/%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88-%ED%86%B5%EA%B3%84%ED%95%99-4-Gibbs-Sampler%EA%B9%81%EC%8A%A4-%EC%83%98%ED%94%8C%EB%9F%AC-%EC%9D%98-%EC%A2%85%EB%A5%98%EC%99%80-%EC%84%B1%EC%A7%88</description>
      </item>
      
      <item>
        <title>What is Bayesian</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb01/</link>
        <pubDate>Thu, 07 Jan 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb01/</guid>
        <description>Chapter 01. Introduction and Examples 본 포스팅은 First Course in Bayesian Statistical Methods를 참고하였다.
이번 장을 통해서는 Likelihood and Prior를 살펴보고 Full probability model의 의미를 보는 데에 주목해보쟈.
베이지안 추론의 목적 우리는 데이터 획득을 통해, 모집단 특성에 대한 불확실성을 줄여나가고자 한다. 이때, 불확실성 정도의 변화 수준을 계량화하는 것이 베이지안 추론통계의 목적이라고 할 수 있다.
핵심 개념  prior distribution $p(\theta)$  사전확률 모수에 대해 기존에 갖고 있던 믿음의 정도   sampling model $p(y|\theta)$  일종의 가능도 함수(likelihood) 사전확률이 참이라는 가정 하에, 특정 데이터가 관찰된 확률   posterior distribution $p(\theta|y)$  데이터가 관찰되었을 때, 이를 바탕으로 수정된 모수에 대한 믿음의 정도    Bayes&#39; Rule $$p(\theta|y) = \frac{p(y|\theta)p(\theta)}{\int_{\Theta}p(y|\tilde{\theta})p(\tilde{\theta})d\tilde{\theta}}$$</description>
      </item>
      
      <item>
        <title>Exchangeability</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb02/</link>
        <pubDate>Thu, 07 Jan 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb02/</guid>
        <description>Chapter 02. Belief, Probability and Exchangeability본 포스팅은 First Course in Bayesian Statistical Methods를 참고하였다.이번 장의 목표는 independence와 exchangeability를 이해하는 것이다. 이를 바탕으로 de Finetti’s theorem이 Bayesian에 갖는 의의를 이해한다면, 베이즈 통계를 공부할 준비가 된 것이다.
Belief functions and Probabilities$Be()$는 belief function이라고 하자. 예를 들어, $Be(F) &amp;gt; Be(G)$는 G보다 F를 더 믿는다고 해석하면 된다. F, G, H를 아래와 같은 각각의 상황이라고 가정해보자.
F : 좌파 후보자를 투표하는 경우 G : 소득이 하위 10%에 속하는 경우 H : 대도시에 거주하는 경우</description>
      </item>
      
      <item>
        <title>Conjugacy</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb03/</link>
        <pubDate>Thu, 07 Jan 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb03/</guid>
        <description>Chapter 03. One-parameter Models 본 포스팅은 First Course in Bayesian Statistical Methods를 참고하였다.
Binomial Model 이항분포:
Prior: Likelihood:
Posterior:
Poisson Model 포아송분포:
Prior:
Likelihood:
Posterior:
Exponential Family Conjugate Prior Conclusion Conjugacy를 잘 알아두자. 혹시 궁금한 점이나 잘못된 내용이 있다면, 댓글로 알려주시면 적극 반영하도록 하겠습니다. </description>
      </item>
      
      <item>
        <title>Monte Carlo Method</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb04/</link>
        <pubDate>Thu, 21 Jan 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb04/</guid>
        <description>Chapter 04. Monte Carlo Approximation 본 포스팅은 First Course in Bayesian Statistical Methods를 참고하였다.
Monte Carlo Method Sampling from Predictive Distributions Posterior Predictive Model Checking 혹시 궁금한 점이나 잘못된 내용이 있다면, 댓글로 알려주시면 적극 반영하도록 하겠습니다. </description>
      </item>
      
      <item>
        <title>Normal Model</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb05/</link>
        <pubDate>Thu, 21 Jan 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb05/</guid>
        <description>Chapter 05. Normal Model 본 포스팅은 First Course in Bayesian Statistical Methods와 Bayesian Data Analysis를 참고하였다.
Warm up! Gamma Distribution Inverse Gamma Distribution Scaled Inverse Chi-squared Distribution Single Parameter Conjugacy unknown ${\sigma}^2$ unknown ${\mu}$ Two Parameter Conjugacy unknown ${\sigma}^2, {\mu}$ semi-conjugacy 혹시 궁금한 점이나 잘못된 내용이 있다면, 댓글로 알려주시면 적극 반영하도록 하겠습니다. </description>
      </item>
      
      <item>
        <title>Gibbs Sampling</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb06/</link>
        <pubDate>Thu, 21 Jan 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb06/</guid>
        <description>Chapter 06. Posterior Approximation with the Gibbs sampler 본 포스팅은 First Course in Bayesian Statistical Methods를 참고하였다.
1. A Semi-conjugate prior distribution 2. Discrete approximations 3. Sampling from the conditional distributions 4. Gibbs Sampling 5. General properties of the Gibbs sampler 6. Introduction to MCMC diagnostics Conclusion semi-conjugate 분포를 모두 알면 full conditional probability를 아는 것과 거의 같다. 혹시 궁금한 점이나 잘못된 내용이 있다면, 댓글로 알려주시면 적극 반영하도록 하겠습니다. </description>
      </item>
      
      <item>
        <title>MVN</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb07/</link>
        <pubDate>Sun, 31 Jan 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb07/</guid>
        <description>Chapter 07. The Multivariate Normal Model 본 포스팅은 First Course in Bayesian Statistical Methods를 참고하였다.
1. Multivariate Normal Desity 2. Semiconjugate prior distribution for the mean 3. inverse-Wishart Distribution 4. Gibbs Sampling of the mean and covariance 4-1. Draw yourself Figure 7.1 1 2 3 4 5  library(tidyverse) library(gridExtra) library(MASS) library(reshape2) library(ash)   1 2 3 4 5 6 7 8 9 10 11  # 초기 설정 inv &amp;lt;- solve MU = matrix(c(50,50), ncol=1) SIGMA = matrix(c(64,0,0,144), ncol=2) # MVN pdf calc.</description>
      </item>
      
      <item>
        <title>BDA Example</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/bayesian/bda_example/</link>
        <pubDate>Wed, 20 Jan 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/bayesian/bda_example/</guid>
        <description>y &amp;lt;- c(93, 112, 122, 135, 122, 150, 118, 90, 124, 114)n &amp;lt;- length(y)s2 &amp;lt;- var(y)my &amp;lt;- mean(y) # helper functions to sample from and evaluate# scaled inverse chi-squared distributionrsinvchisq &amp;lt;- function(n, nu, s2, ...) nu*s2 / rchisq(n , nu, ...)dsinvchisq &amp;lt;- function(x, nu, s2){exp(log(nu/2)*nu/2 - lgamma(nu/2) + log(s2)/2*nu - log(x)*(nu/2+1) - (nu*s2/2)/x)}ns &amp;lt;- 1000sigma2 &amp;lt;- rsinvchisq(ns, n-1, s2)mu &amp;lt;- my + sqrt(sigma2/n)*rnorm(length(sigma2))sigma &amp;lt;- sqrt(sigma2)ynew &amp;lt;- rnorm(ns, mu, sigma)t1l &amp;lt;- c(90, 150)t2l &amp;lt;- c(10, 60)nl &amp;lt;- c(50, 185)t1 &amp;lt;- seq(t1l[1], t1l[2], length.</description>
      </item>
      
    
  </channel>
</rss>