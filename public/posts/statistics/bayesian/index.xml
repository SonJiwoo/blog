<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bayesian on JW Blog</title>
    <link>https://jiwooblog.netlify.app/posts/statistics/bayesian/</link>
    <description>Recent content in Bayesian on JW Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ko</language>
    <copyright>&amp;copy;{year}, All Rights Reserved</copyright>
    <lastBuildDate>Fri, 08 Jan 2021 10:08:56 +0900</lastBuildDate>
    
        <atom:link href="https://jiwooblog.netlify.app/posts/statistics/bayesian/index.xml" rel="self" type="application/rss+xml" />
    
    
    
      
      <item>
        <title>베이즈 통계란</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb01/</link>
        <pubDate>Thu, 07 Jan 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb01/</guid>
        <description>Chapter 01. Introduction and Examples 본 포스팅은 First Course in Bayesian Statistical Methods를 참고하였다.
이번 장을 통해서는 Likelihood and Prior를 살펴보고 Full probability model의 의미를 보는 데에 주목해보쟈.
베이지안 추론의 목적 우리는 데이터 획득을 통해, 모집단 특성에 대한 불확실성을 줄여나가고자 한다. 이때, 불확실성 정도의 변화 수준을 계량화하는 것이 베이지안 추론통계의 목적이라고 할 수 있다.
핵심 개념  prior distribution $p(\theta)$  사전확률 모수에 대해 기존에 갖고 있던 믿음의 정도   sampling model $p(y|\theta)$  일종의 가능도 함수(likelihood) 사전확률이 참이라는 가정 하에, 특정 데이터가 관찰된 확률   posterior distribution $p(\theta|y)$  데이터가 관찰되었을 때, 이를 바탕으로 수정된 모수에 대한 믿음의 정도    Bayes&#39; Rule $$p(\theta|y) = \frac{p(y|\theta)p(\theta)}{\int_{\Theta}p(y|\tilde{\theta})p(\tilde{\theta})d\tilde{\theta}}$$</description>
      </item>
      
      <item>
        <title>Exchangeability</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb02/</link>
        <pubDate>Thu, 07 Jan 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb02/</guid>
        <description>Chapter 02. Belief, Probability and Exchangeability본 포스팅은 First Course in Bayesian Statistical Methods를 참고하였다.이번 장의 목표는 independence와 exchangeability를 이해하는 것이다. 이를 바탕으로 de Finetti’s theorem이 Bayesian에 갖는 의의를 이해한다면, 베이즈 통계를 공부할 준비가 된 것이다.
Belief functions and Probabilities$Be()$는 belief function이라고 하자. 예를 들어, $Be(F) &amp;gt; Be(G)$는 G보다 F를 더 믿는다고 해석하면 된다. F, G, H를 아래와 같은 각각의 상황이라고 가정해보자.
F : 좌파 후보자를 투표하는 경우 G : 소득이 하위 10%에 속하는 경우 H : 대도시에 거주하는 경우</description>
      </item>
      
      <item>
        <title>One-parameter Model</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb03/</link>
        <pubDate>Thu, 07 Jan 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb03/</guid>
        <description>Chapter 03. One-parameter Models 본 포스팅은 First Course in Bayesian Statistical Methods를 참고하였다.
Binomial Model 이항분포:
Prior: Likelihood:
Posterior:
Poisson Model 포아송분포:
Prior:
Likelihood:
Posterior:
Exponential Family Conjugate Prior Conclusion Conjugacy를 잘 알아두자. 혹시 궁금한 점이나 잘못된 내용이 있다면, 댓글로 알려주시면 적극 반영하도록 하겠습니다. </description>
      </item>
      
      <item>
        <title>Monte Carlo Approximation</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb04/</link>
        <pubDate>Thu, 21 Jan 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb04/</guid>
        <description>Chapter 04. Monte Carlo Approximation 본 포스팅은 First Course in Bayesian Statistical Methods를 참고하였다.
Monte Carlo Method Sampling from Predictive Distributions Posterior Predictive Model Checking 혹시 궁금한 점이나 잘못된 내용이 있다면, 댓글로 알려주시면 적극 반영하도록 하겠습니다. </description>
      </item>
      
      <item>
        <title>Normal Model</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb05/</link>
        <pubDate>Thu, 21 Jan 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/bayesian/fcb05/</guid>
        <description>Chapter 05. Normal Model 본 포스팅은 First Course in Bayesian Statistical Methods와 Bayesian Data Analysis를 참고하였다.
Warm up! Gamma Distribution Inverse Gamma Distribution Scaled Inverse Chi-squared Distribution Single Parameter Conjugacy unknown ${\sigma}^2$ unknown ${\mu}$ Two Parameter Conjugacy unknown ${\sigma}^2, {\mu}$ semi-conjugacy 혹시 궁금한 점이나 잘못된 내용이 있다면, 댓글로 알려주시면 적극 반영하도록 하겠습니다. </description>
      </item>
      
      <item>
        <title>BDA Example</title>
        <link>https://jiwooblog.netlify.app/posts/statistics/bayesian/bda_example/</link>
        <pubDate>Wed, 20 Jan 2021 10:08:56 +0900</pubDate>
        
        <guid>https://jiwooblog.netlify.app/posts/statistics/bayesian/bda_example/</guid>
        <description>y &amp;lt;- c(93, 112, 122, 135, 122, 150, 118, 90, 124, 114)n &amp;lt;- length(y)s2 &amp;lt;- var(y)my &amp;lt;- mean(y) # helper functions to sample from and evaluate# scaled inverse chi-squared distributionrsinvchisq &amp;lt;- function(n, nu, s2, ...) nu*s2 / rchisq(n , nu, ...)dsinvchisq &amp;lt;- function(x, nu, s2){exp(log(nu/2)*nu/2 - lgamma(nu/2) + log(s2)/2*nu - log(x)*(nu/2+1) - (nu*s2/2)/x)}ns &amp;lt;- 1000sigma2 &amp;lt;- rsinvchisq(ns, n-1, s2)mu &amp;lt;- my + sqrt(sigma2/n)*rnorm(length(sigma2))sigma &amp;lt;- sqrt(sigma2)ynew &amp;lt;- rnorm(ns, mu, sigma)t1l &amp;lt;- c(90, 150)t2l &amp;lt;- c(10, 60)nl &amp;lt;- c(50, 185)t1 &amp;lt;- seq(t1l[1], t1l[2], length.</description>
      </item>
      
    
  </channel>
</rss>